{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bythyag/overfitting-analysis/blob/main/scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfaQi73hObSp"
      },
      "source": [
        "Experiment: Overfitting, Underfitting, Regularization (L1, L2, ElasticNet), Early Stopping\n",
        "\n",
        "Dataset: CIFAR-10\n",
        "Model: Custom CNN\n",
        "Framework: PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RnnP8sbGN9ss"
      },
      "outputs": [],
      "source": [
        "#load libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import copy\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSavXujaOT6v",
        "outputId": "6326c4d6-f75d-491d-e6f5-c25d6c53d49e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Move to GPU\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Data parameters\n",
        "BATCH_SIZE = 128\n",
        "VALIDATION_SPLIT = 0.1 # 10% of training data for validation\n",
        "\n",
        "# Training parameters\n",
        "EPOCHS = 100 # Increase epochs to ensure overfitting can be observed\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Regularization parameters\n",
        "WEIGHT_DECAY_L2 = 1e-4 # Lambda for L2\n",
        "L1_LAMBDA = 1e-5      # Lambda for L1\n",
        "# Elastic Net uses both WEIGHT_DECAY_L2 and L1_LAMBDA\n",
        "\n",
        "# Early Stopping parameters\n",
        "EARLY_STOPPING_PATIENCE = 7\n",
        "EARLY_STOPPING_MIN_DELTA = 0.001 # Minimum change to qualify as improvement\n",
        "\n",
        "# For reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0S5sWTNOrtj",
        "outputId": "fb0a5800-9f8f-406b-9ad6-0361660b873a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Loading Data ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full training set size: 50000\n",
            "Training subset size: 45000\n",
            "Validation subset size: 5000\n",
            "Test set size: 10000\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Loading Data ---\")\n",
        "\n",
        "# Standard transformations for CIFAR-10\n",
        "# Normalization values are standard for CIFAR-10\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(), # Basic augmentation\n",
        "    transforms.RandomCrop(32, padding=4), # Basic augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "full_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                  download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                                download=True, transform=transform_test)\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "num_train = len(full_train_dataset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(VALIDATION_SPLIT * num_train))\n",
        "np.random.shuffle(indices) # Shuffle indices\n",
        "\n",
        "train_idx, val_idx = indices[split:], indices[:split]\n",
        "train_subset = Subset(full_train_dataset, train_idx)\n",
        "val_subset = Subset(full_train_dataset, val_idx)\n",
        "\n",
        "# Adjust validation subset to use test transform (no augmentation)\n",
        "# Create a new validation dataset instance with the correct indices and transform\n",
        "# Note: This is slightly less efficient than modifying Subset, but cleaner\n",
        "val_dataset_clean = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                                  download=False, transform=transform_test)\n",
        "val_subset_eval = Subset(val_dataset_clean, val_idx)\n",
        "\n",
        "\n",
        "print(f\"Full training set size: {len(full_train_dataset)}\")\n",
        "print(f\"Training subset size: {len(train_subset)}\")\n",
        "print(f\"Validation subset size: {len(val_subset_eval)}\")\n",
        "print(f\"Test set size: {len(test_dataset)}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_subset_eval, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCK6DN9wO0YF",
        "outputId": "0e040e72-579e-4e22-e705-49485c8b4197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Defining Models ---\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Defining Models ---\")\n",
        "\n",
        "# Model capable of overfitting CIFAR-10\n",
        "class OverfittingCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(OverfittingCNN, self).__init__()\n",
        "        self.conv_layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # 32x32 -> 16x16\n",
        "        )\n",
        "        self.conv_layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # 16x16 -> 8x8\n",
        "        )\n",
        "        self.conv_layer3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # 8x8 -> 4x4\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 4 * 4, 1024), # 256 channels * 4x4 feature map\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # nn.Dropout(0.5), # Could add dropout, but focusing on L1/L2/ElasticNet\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer1(x)\n",
        "        x = self.conv_layer2(x)\n",
        "        x = self.conv_layer3(x)\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "\n",
        "# Simpler model for demonstrating Underfitting\n",
        "class UnderfittingCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(UnderfittingCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2) # 32 -> 16\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        # Pool again: 16 -> 8\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 64) # Flattened size\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifD3ycghO38d",
        "outputId": "85f4614d-0c2f-434f-df0d-fa21502e5c6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Defining Training & Evaluation ---\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Defining Training & Evaluation ---\")\n",
        "\n",
        "def calculate_l1_loss(model):\n",
        "    \"\"\"Calculates the L1 norm of model parameters (weights only).\"\"\"\n",
        "    l1_norm = sum(p.abs().sum() for name, p in model.named_parameters() if 'weight' in name)\n",
        "    return l1_norm\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    \"\"\"Evaluates the model on the given data loader.\"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs, device,\n",
        "                l1_lambda=0.0, early_stopping_patience=None, early_stopping_min_delta=0.0,\n",
        "                model_name=\"Model\"):\n",
        "    \"\"\"Trains the model with options for L1 regularization and early stopping.\"\"\"\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    best_model_state = None\n",
        "    stop_epoch = epochs\n",
        "\n",
        "    print(f\"\\n--- Training {model_name} ---\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            base_loss = criterion(outputs, labels)\n",
        "\n",
        "            # Add L1 regularization loss (if applicable)\n",
        "            l1_loss = 0.0\n",
        "            if l1_lambda > 0:\n",
        "                l1_loss = l1_lambda * calculate_l1_loss(model)\n",
        "\n",
        "            loss = base_loss + l1_loss\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            if (i + 1) % 100 == 0: # Print progress every 100 batches\n",
        "                 print(f'Epoch [{epoch+1}/{epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_acc = correct_train / total_train\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "\n",
        "        # Validation step\n",
        "        epoch_val_loss, epoch_val_acc = evaluate_model(model, val_loader, criterion, device)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} | \"\n",
        "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
        "\n",
        "        # Early Stopping check\n",
        "        if early_stopping_patience is not None:\n",
        "            # Check if validation loss improved\n",
        "            if epoch_val_loss < best_val_loss - early_stopping_min_delta:\n",
        "                best_val_loss = epoch_val_loss\n",
        "                epochs_no_improve = 0\n",
        "                # Save the best model state\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "                print(f\"  -> Validation loss improved to {best_val_loss:.4f}. Saving model.\")\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                print(f\"  -> Validation loss did not improve for {epochs_no_improve} epoch(s).\")\n",
        "\n",
        "            if epochs_no_improve >= early_stopping_patience:\n",
        "                print(f\"\\nEarly stopping triggered after epoch {epoch+1}!\")\n",
        "                stop_epoch = epoch + 1\n",
        "                # Load the best model state found\n",
        "                if best_model_state is not None:\n",
        "                    model.load_state_dict(best_model_state)\n",
        "                    print(\"  -> Restored best model weights.\")\n",
        "                else:\n",
        "                     print(\"  -> Warning: Early stopping triggered, but no best model state was saved.\")\n",
        "                break # Exit the training loop\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"--- Training Finished for {model_name} in {(end_time - start_time)/60:.2f} minutes ---\")\n",
        "\n",
        "    # If early stopping didn't trigger, the last model is the 'best' in terms of epochs completed\n",
        "    # If it did trigger, the best model state is already loaded.\n",
        "\n",
        "    return model, history, stop_epoch # Return trained model and history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLQpHMP4PIkh",
        "outputId": "db5b6f5e-82e3-4c8b-bedf-17f5bc326972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Experiment 1: Underfitting ===\n",
            "\n",
            "--- Training Underfitting ---\n",
            "Epoch [1/100], Batch [100/352], Loss: 1.7056\n",
            "Epoch [1/100], Batch [200/352], Loss: 1.4665\n",
            "Epoch [1/100], Batch [300/352], Loss: 1.4363\n",
            "Epoch 1/100 | Train Loss: 1.6597, Train Acc: 0.3951 | Val Loss: 1.4182, Val Acc: 0.4892\n",
            "Epoch [2/100], Batch [100/352], Loss: 1.5268\n",
            "Epoch [2/100], Batch [200/352], Loss: 1.3900\n",
            "Epoch [2/100], Batch [300/352], Loss: 1.2421\n",
            "Epoch 2/100 | Train Loss: 1.3814, Train Acc: 0.5052 | Val Loss: 1.2248, Val Acc: 0.5720\n",
            "Epoch [3/100], Batch [100/352], Loss: 1.3527\n",
            "Epoch [3/100], Batch [200/352], Loss: 1.4779\n",
            "Epoch [3/100], Batch [300/352], Loss: 1.2782\n",
            "Epoch 3/100 | Train Loss: 1.2677, Train Acc: 0.5492 | Val Loss: 1.1355, Val Acc: 0.5994\n",
            "Epoch [4/100], Batch [100/352], Loss: 1.1106\n",
            "Epoch [4/100], Batch [200/352], Loss: 1.0811\n",
            "Epoch [4/100], Batch [300/352], Loss: 1.0783\n",
            "Epoch 4/100 | Train Loss: 1.1814, Train Acc: 0.5801 | Val Loss: 1.0519, Val Acc: 0.6216\n",
            "Epoch [5/100], Batch [100/352], Loss: 1.1142\n",
            "Epoch [5/100], Batch [200/352], Loss: 0.9588\n",
            "Epoch [5/100], Batch [300/352], Loss: 1.0760\n",
            "Epoch 5/100 | Train Loss: 1.1243, Train Acc: 0.6005 | Val Loss: 1.0277, Val Acc: 0.6316\n",
            "Epoch [6/100], Batch [100/352], Loss: 1.1345\n",
            "Epoch [6/100], Batch [200/352], Loss: 1.0428\n",
            "Epoch [6/100], Batch [300/352], Loss: 1.0933\n",
            "Epoch 6/100 | Train Loss: 1.0883, Train Acc: 0.6137 | Val Loss: 0.9963, Val Acc: 0.6524\n",
            "Epoch [7/100], Batch [100/352], Loss: 0.9934\n",
            "Epoch [7/100], Batch [200/352], Loss: 1.0824\n",
            "Epoch [7/100], Batch [300/352], Loss: 1.0173\n",
            "Epoch 7/100 | Train Loss: 1.0479, Train Acc: 0.6298 | Val Loss: 0.9694, Val Acc: 0.6590\n",
            "Epoch [8/100], Batch [100/352], Loss: 1.0587\n",
            "Epoch [8/100], Batch [200/352], Loss: 1.0869\n",
            "Epoch [8/100], Batch [300/352], Loss: 1.0237\n",
            "Epoch 8/100 | Train Loss: 1.0274, Train Acc: 0.6391 | Val Loss: 0.9616, Val Acc: 0.6562\n",
            "Epoch [9/100], Batch [100/352], Loss: 1.0946\n",
            "Epoch [9/100], Batch [200/352], Loss: 1.0158\n",
            "Epoch [9/100], Batch [300/352], Loss: 1.0169\n",
            "Epoch 9/100 | Train Loss: 1.0059, Train Acc: 0.6464 | Val Loss: 0.9466, Val Acc: 0.6640\n",
            "Epoch [10/100], Batch [100/352], Loss: 1.1028\n",
            "Epoch [10/100], Batch [200/352], Loss: 1.1310\n",
            "Epoch [10/100], Batch [300/352], Loss: 1.1009\n",
            "Epoch 10/100 | Train Loss: 0.9859, Train Acc: 0.6514 | Val Loss: 0.9122, Val Acc: 0.6754\n",
            "Epoch [11/100], Batch [100/352], Loss: 0.9347\n",
            "Epoch [11/100], Batch [200/352], Loss: 0.9439\n",
            "Epoch [11/100], Batch [300/352], Loss: 0.9383\n",
            "Epoch 11/100 | Train Loss: 0.9688, Train Acc: 0.6584 | Val Loss: 0.9349, Val Acc: 0.6722\n",
            "Epoch [12/100], Batch [100/352], Loss: 0.9425\n",
            "Epoch [12/100], Batch [200/352], Loss: 0.9549\n",
            "Epoch [12/100], Batch [300/352], Loss: 1.1368\n",
            "Epoch 12/100 | Train Loss: 0.9530, Train Acc: 0.6616 | Val Loss: 0.9187, Val Acc: 0.6792\n",
            "Epoch [13/100], Batch [100/352], Loss: 0.9845\n",
            "Epoch [13/100], Batch [200/352], Loss: 1.0888\n",
            "Epoch [13/100], Batch [300/352], Loss: 0.8620\n",
            "Epoch 13/100 | Train Loss: 0.9374, Train Acc: 0.6694 | Val Loss: 0.9148, Val Acc: 0.6744\n",
            "Epoch [14/100], Batch [100/352], Loss: 0.8824\n",
            "Epoch [14/100], Batch [200/352], Loss: 0.7799\n",
            "Epoch [14/100], Batch [300/352], Loss: 0.8704\n",
            "Epoch 14/100 | Train Loss: 0.9220, Train Acc: 0.6769 | Val Loss: 0.8573, Val Acc: 0.7006\n",
            "Epoch [15/100], Batch [100/352], Loss: 0.8790\n",
            "Epoch [15/100], Batch [200/352], Loss: 0.9373\n",
            "Epoch [15/100], Batch [300/352], Loss: 0.9705\n",
            "Epoch 15/100 | Train Loss: 0.9163, Train Acc: 0.6774 | Val Loss: 0.8875, Val Acc: 0.6858\n",
            "Epoch [16/100], Batch [100/352], Loss: 0.9060\n",
            "Epoch [16/100], Batch [200/352], Loss: 0.9856\n",
            "Epoch [16/100], Batch [300/352], Loss: 0.7895\n",
            "Epoch 16/100 | Train Loss: 0.9000, Train Acc: 0.6827 | Val Loss: 0.8599, Val Acc: 0.6930\n",
            "Epoch [17/100], Batch [100/352], Loss: 0.8071\n",
            "Epoch [17/100], Batch [200/352], Loss: 0.8038\n",
            "Epoch [17/100], Batch [300/352], Loss: 0.8633\n",
            "Epoch 17/100 | Train Loss: 0.8890, Train Acc: 0.6861 | Val Loss: 0.8502, Val Acc: 0.6990\n",
            "Epoch [18/100], Batch [100/352], Loss: 0.9168\n",
            "Epoch [18/100], Batch [200/352], Loss: 0.9243\n",
            "Epoch [18/100], Batch [300/352], Loss: 0.9009\n",
            "Epoch 18/100 | Train Loss: 0.8791, Train Acc: 0.6914 | Val Loss: 0.8356, Val Acc: 0.7068\n",
            "Epoch [19/100], Batch [100/352], Loss: 0.9022\n",
            "Epoch [19/100], Batch [200/352], Loss: 0.8925\n",
            "Epoch [19/100], Batch [300/352], Loss: 0.7850\n",
            "Epoch 19/100 | Train Loss: 0.8715, Train Acc: 0.6932 | Val Loss: 0.8115, Val Acc: 0.7140\n",
            "Epoch [20/100], Batch [100/352], Loss: 0.9451\n",
            "Epoch [20/100], Batch [200/352], Loss: 0.8653\n",
            "Epoch [20/100], Batch [300/352], Loss: 0.7725\n",
            "Epoch 20/100 | Train Loss: 0.8560, Train Acc: 0.6998 | Val Loss: 0.8150, Val Acc: 0.7158\n",
            "Epoch [21/100], Batch [100/352], Loss: 0.7991\n",
            "Epoch [21/100], Batch [200/352], Loss: 0.7742\n",
            "Epoch [21/100], Batch [300/352], Loss: 0.6715\n",
            "Epoch 21/100 | Train Loss: 0.8524, Train Acc: 0.7015 | Val Loss: 0.8246, Val Acc: 0.7078\n",
            "Epoch [22/100], Batch [100/352], Loss: 0.9864\n",
            "Epoch [22/100], Batch [200/352], Loss: 0.7435\n",
            "Epoch [22/100], Batch [300/352], Loss: 0.8306\n",
            "Epoch 22/100 | Train Loss: 0.8406, Train Acc: 0.7050 | Val Loss: 0.8087, Val Acc: 0.7124\n",
            "Epoch [23/100], Batch [100/352], Loss: 0.7788\n",
            "Epoch [23/100], Batch [200/352], Loss: 0.9553\n",
            "Epoch [23/100], Batch [300/352], Loss: 1.0190\n",
            "Epoch 23/100 | Train Loss: 0.8335, Train Acc: 0.7061 | Val Loss: 0.7754, Val Acc: 0.7332\n",
            "Epoch [24/100], Batch [100/352], Loss: 0.8669\n",
            "Epoch [24/100], Batch [200/352], Loss: 0.8994\n",
            "Epoch [24/100], Batch [300/352], Loss: 0.8751\n",
            "Epoch 24/100 | Train Loss: 0.8224, Train Acc: 0.7112 | Val Loss: 0.7840, Val Acc: 0.7302\n",
            "Epoch [25/100], Batch [100/352], Loss: 0.7645\n",
            "Epoch [25/100], Batch [200/352], Loss: 0.8720\n",
            "Epoch [25/100], Batch [300/352], Loss: 0.7049\n",
            "Epoch 25/100 | Train Loss: 0.8234, Train Acc: 0.7105 | Val Loss: 0.8086, Val Acc: 0.7168\n",
            "Epoch [26/100], Batch [100/352], Loss: 0.8493\n",
            "Epoch [26/100], Batch [200/352], Loss: 0.7971\n",
            "Epoch [26/100], Batch [300/352], Loss: 0.7978\n",
            "Epoch 26/100 | Train Loss: 0.8081, Train Acc: 0.7163 | Val Loss: 0.8228, Val Acc: 0.7140\n",
            "Epoch [27/100], Batch [100/352], Loss: 0.8928\n",
            "Epoch [27/100], Batch [200/352], Loss: 0.7089\n",
            "Epoch [27/100], Batch [300/352], Loss: 0.6912\n",
            "Epoch 27/100 | Train Loss: 0.8092, Train Acc: 0.7145 | Val Loss: 0.7482, Val Acc: 0.7340\n",
            "Epoch [28/100], Batch [100/352], Loss: 0.7988\n",
            "Epoch [28/100], Batch [200/352], Loss: 0.7439\n",
            "Epoch [28/100], Batch [300/352], Loss: 0.6648\n",
            "Epoch 28/100 | Train Loss: 0.7975, Train Acc: 0.7195 | Val Loss: 0.7596, Val Acc: 0.7314\n",
            "Epoch [29/100], Batch [100/352], Loss: 0.5979\n",
            "Epoch [29/100], Batch [200/352], Loss: 0.8408\n",
            "Epoch [29/100], Batch [300/352], Loss: 0.8181\n",
            "Epoch 29/100 | Train Loss: 0.7891, Train Acc: 0.7232 | Val Loss: 0.7617, Val Acc: 0.7336\n",
            "Epoch [30/100], Batch [100/352], Loss: 0.6007\n",
            "Epoch [30/100], Batch [200/352], Loss: 0.5973\n",
            "Epoch [30/100], Batch [300/352], Loss: 0.8500\n",
            "Epoch 30/100 | Train Loss: 0.7874, Train Acc: 0.7237 | Val Loss: 0.7600, Val Acc: 0.7272\n",
            "Epoch [31/100], Batch [100/352], Loss: 0.8263\n",
            "Epoch [31/100], Batch [200/352], Loss: 0.7732\n",
            "Epoch [31/100], Batch [300/352], Loss: 0.7625\n",
            "Epoch 31/100 | Train Loss: 0.7838, Train Acc: 0.7252 | Val Loss: 0.7374, Val Acc: 0.7404\n",
            "Epoch [32/100], Batch [100/352], Loss: 0.7835\n",
            "Epoch [32/100], Batch [200/352], Loss: 0.6606\n",
            "Epoch [32/100], Batch [300/352], Loss: 0.8911\n",
            "Epoch 32/100 | Train Loss: 0.7730, Train Acc: 0.7288 | Val Loss: 0.7472, Val Acc: 0.7376\n",
            "Epoch [33/100], Batch [100/352], Loss: 0.8500\n",
            "Epoch [33/100], Batch [200/352], Loss: 0.6998\n",
            "Epoch [33/100], Batch [300/352], Loss: 0.7700\n",
            "Epoch 33/100 | Train Loss: 0.7738, Train Acc: 0.7272 | Val Loss: 0.7257, Val Acc: 0.7464\n",
            "Epoch [34/100], Batch [100/352], Loss: 0.6867\n",
            "Epoch [34/100], Batch [200/352], Loss: 0.8188\n",
            "Epoch [34/100], Batch [300/352], Loss: 0.7500\n",
            "Epoch 34/100 | Train Loss: 0.7681, Train Acc: 0.7310 | Val Loss: 0.7582, Val Acc: 0.7390\n",
            "Epoch [35/100], Batch [100/352], Loss: 0.7535\n",
            "Epoch [35/100], Batch [200/352], Loss: 0.7811\n",
            "Epoch [35/100], Batch [300/352], Loss: 0.8025\n",
            "Epoch 35/100 | Train Loss: 0.7693, Train Acc: 0.7317 | Val Loss: 0.7298, Val Acc: 0.7462\n",
            "Epoch [36/100], Batch [100/352], Loss: 0.8370\n",
            "Epoch [36/100], Batch [200/352], Loss: 0.7292\n",
            "Epoch [36/100], Batch [300/352], Loss: 0.8254\n",
            "Epoch 36/100 | Train Loss: 0.7594, Train Acc: 0.7332 | Val Loss: 0.7546, Val Acc: 0.7448\n",
            "Epoch [37/100], Batch [100/352], Loss: 0.7966\n",
            "Epoch [37/100], Batch [200/352], Loss: 0.6066\n",
            "Epoch [37/100], Batch [300/352], Loss: 0.7705\n",
            "Epoch 37/100 | Train Loss: 0.7568, Train Acc: 0.7340 | Val Loss: 0.7261, Val Acc: 0.7512\n",
            "Epoch [38/100], Batch [100/352], Loss: 0.6757\n",
            "Epoch [38/100], Batch [200/352], Loss: 0.8709\n",
            "Epoch [38/100], Batch [300/352], Loss: 0.6451\n",
            "Epoch 38/100 | Train Loss: 0.7538, Train Acc: 0.7354 | Val Loss: 0.7269, Val Acc: 0.7522\n",
            "Epoch [39/100], Batch [100/352], Loss: 1.0459\n",
            "Epoch [39/100], Batch [200/352], Loss: 0.6941\n",
            "Epoch [39/100], Batch [300/352], Loss: 0.7236\n",
            "Epoch 39/100 | Train Loss: 0.7524, Train Acc: 0.7346 | Val Loss: 0.7426, Val Acc: 0.7392\n",
            "Epoch [40/100], Batch [100/352], Loss: 0.6720\n",
            "Epoch [40/100], Batch [200/352], Loss: 0.6208\n",
            "Epoch [40/100], Batch [300/352], Loss: 0.8056\n",
            "Epoch 40/100 | Train Loss: 0.7423, Train Acc: 0.7377 | Val Loss: 0.7142, Val Acc: 0.7542\n",
            "Epoch [41/100], Batch [100/352], Loss: 0.6969\n",
            "Epoch [41/100], Batch [200/352], Loss: 0.6930\n",
            "Epoch [41/100], Batch [300/352], Loss: 0.6818\n",
            "Epoch 41/100 | Train Loss: 0.7433, Train Acc: 0.7373 | Val Loss: 0.7131, Val Acc: 0.7534\n",
            "Epoch [42/100], Batch [100/352], Loss: 0.6077\n",
            "Epoch [42/100], Batch [200/352], Loss: 0.6683\n",
            "Epoch [42/100], Batch [300/352], Loss: 0.7893\n",
            "Epoch 42/100 | Train Loss: 0.7447, Train Acc: 0.7395 | Val Loss: 0.7057, Val Acc: 0.7512\n",
            "Epoch [43/100], Batch [100/352], Loss: 0.7943\n",
            "Epoch [43/100], Batch [200/352], Loss: 0.6674\n",
            "Epoch [43/100], Batch [300/352], Loss: 0.7482\n",
            "Epoch 43/100 | Train Loss: 0.7356, Train Acc: 0.7432 | Val Loss: 0.7355, Val Acc: 0.7484\n",
            "Epoch [44/100], Batch [100/352], Loss: 0.6319\n",
            "Epoch [44/100], Batch [200/352], Loss: 0.8613\n",
            "Epoch [44/100], Batch [300/352], Loss: 0.8328\n",
            "Epoch 44/100 | Train Loss: 0.7404, Train Acc: 0.7404 | Val Loss: 0.6970, Val Acc: 0.7624\n",
            "Epoch [45/100], Batch [100/352], Loss: 0.8163\n",
            "Epoch [45/100], Batch [200/352], Loss: 0.7395\n",
            "Epoch [45/100], Batch [300/352], Loss: 0.7141\n",
            "Epoch 45/100 | Train Loss: 0.7346, Train Acc: 0.7439 | Val Loss: 0.7171, Val Acc: 0.7542\n",
            "Epoch [46/100], Batch [100/352], Loss: 0.7482\n",
            "Epoch [46/100], Batch [200/352], Loss: 0.7908\n",
            "Epoch [46/100], Batch [300/352], Loss: 0.7577\n",
            "Epoch 46/100 | Train Loss: 0.7318, Train Acc: 0.7456 | Val Loss: 0.7111, Val Acc: 0.7564\n",
            "Epoch [47/100], Batch [100/352], Loss: 0.7038\n",
            "Epoch [47/100], Batch [200/352], Loss: 0.8620\n",
            "Epoch [47/100], Batch [300/352], Loss: 0.7800\n",
            "Epoch 47/100 | Train Loss: 0.7286, Train Acc: 0.7433 | Val Loss: 0.7217, Val Acc: 0.7538\n",
            "Epoch [48/100], Batch [100/352], Loss: 0.6948\n",
            "Epoch [48/100], Batch [200/352], Loss: 0.5528\n",
            "Epoch [48/100], Batch [300/352], Loss: 0.6526\n",
            "Epoch 48/100 | Train Loss: 0.7254, Train Acc: 0.7455 | Val Loss: 0.7082, Val Acc: 0.7500\n",
            "Epoch [49/100], Batch [100/352], Loss: 0.6835\n",
            "Epoch [49/100], Batch [200/352], Loss: 0.6060\n",
            "Epoch [49/100], Batch [300/352], Loss: 0.7896\n",
            "Epoch 49/100 | Train Loss: 0.7226, Train Acc: 0.7458 | Val Loss: 0.6987, Val Acc: 0.7572\n",
            "Epoch [50/100], Batch [100/352], Loss: 0.8490\n",
            "Epoch [50/100], Batch [200/352], Loss: 0.7416\n",
            "Epoch [50/100], Batch [300/352], Loss: 0.6093\n",
            "Epoch 50/100 | Train Loss: 0.7205, Train Acc: 0.7477 | Val Loss: 0.7221, Val Acc: 0.7488\n",
            "Epoch [51/100], Batch [100/352], Loss: 0.7744\n",
            "Epoch [51/100], Batch [200/352], Loss: 0.7686\n",
            "Epoch [51/100], Batch [300/352], Loss: 0.6790\n",
            "Epoch 51/100 | Train Loss: 0.7193, Train Acc: 0.7471 | Val Loss: 0.7000, Val Acc: 0.7570\n",
            "Epoch [52/100], Batch [100/352], Loss: 0.6779\n",
            "Epoch [52/100], Batch [200/352], Loss: 0.7464\n",
            "Epoch [52/100], Batch [300/352], Loss: 0.5548\n",
            "Epoch 52/100 | Train Loss: 0.7124, Train Acc: 0.7478 | Val Loss: 0.6929, Val Acc: 0.7600\n",
            "Epoch [53/100], Batch [100/352], Loss: 0.8036\n",
            "Epoch [53/100], Batch [200/352], Loss: 0.7882\n",
            "Epoch [53/100], Batch [300/352], Loss: 0.6841\n",
            "Epoch 53/100 | Train Loss: 0.7110, Train Acc: 0.7507 | Val Loss: 0.6874, Val Acc: 0.7574\n",
            "Epoch [54/100], Batch [100/352], Loss: 0.5912\n",
            "Epoch [54/100], Batch [200/352], Loss: 0.8160\n",
            "Epoch [54/100], Batch [300/352], Loss: 0.6622\n",
            "Epoch 54/100 | Train Loss: 0.7168, Train Acc: 0.7475 | Val Loss: 0.6869, Val Acc: 0.7600\n",
            "Epoch [55/100], Batch [100/352], Loss: 0.6636\n",
            "Epoch [55/100], Batch [200/352], Loss: 0.7820\n",
            "Epoch [55/100], Batch [300/352], Loss: 0.7210\n",
            "Epoch 55/100 | Train Loss: 0.7114, Train Acc: 0.7507 | Val Loss: 0.7022, Val Acc: 0.7600\n",
            "Epoch [56/100], Batch [100/352], Loss: 0.7594\n",
            "Epoch [56/100], Batch [200/352], Loss: 0.6940\n",
            "Epoch [56/100], Batch [300/352], Loss: 0.6423\n",
            "Epoch 56/100 | Train Loss: 0.7115, Train Acc: 0.7509 | Val Loss: 0.6886, Val Acc: 0.7590\n",
            "Epoch [57/100], Batch [100/352], Loss: 0.7626\n",
            "Epoch [57/100], Batch [200/352], Loss: 0.6470\n",
            "Epoch [57/100], Batch [300/352], Loss: 0.8471\n",
            "Epoch 57/100 | Train Loss: 0.7080, Train Acc: 0.7507 | Val Loss: 0.7115, Val Acc: 0.7526\n",
            "Epoch [58/100], Batch [100/352], Loss: 0.8259\n",
            "Epoch [58/100], Batch [200/352], Loss: 0.8954\n",
            "Epoch [58/100], Batch [300/352], Loss: 0.6364\n",
            "Epoch 58/100 | Train Loss: 0.6999, Train Acc: 0.7550 | Val Loss: 0.6916, Val Acc: 0.7608\n",
            "Epoch [59/100], Batch [100/352], Loss: 0.7746\n",
            "Epoch [59/100], Batch [200/352], Loss: 0.8171\n",
            "Epoch [59/100], Batch [300/352], Loss: 0.7188\n",
            "Epoch 59/100 | Train Loss: 0.7110, Train Acc: 0.7511 | Val Loss: 0.6921, Val Acc: 0.7656\n",
            "Epoch [60/100], Batch [100/352], Loss: 0.5849\n",
            "Epoch [60/100], Batch [200/352], Loss: 0.7001\n",
            "Epoch [60/100], Batch [300/352], Loss: 0.6142\n",
            "Epoch 60/100 | Train Loss: 0.7013, Train Acc: 0.7542 | Val Loss: 0.6790, Val Acc: 0.7660\n",
            "Epoch [61/100], Batch [100/352], Loss: 0.6715\n",
            "Epoch [61/100], Batch [200/352], Loss: 0.7089\n",
            "Epoch [61/100], Batch [300/352], Loss: 0.7259\n",
            "Epoch 61/100 | Train Loss: 0.6961, Train Acc: 0.7570 | Val Loss: 0.6817, Val Acc: 0.7702\n",
            "Epoch [62/100], Batch [100/352], Loss: 0.6334\n",
            "Epoch [62/100], Batch [200/352], Loss: 0.6132\n",
            "Epoch [62/100], Batch [300/352], Loss: 0.7301\n",
            "Epoch 62/100 | Train Loss: 0.6983, Train Acc: 0.7534 | Val Loss: 0.6865, Val Acc: 0.7596\n",
            "Epoch [63/100], Batch [100/352], Loss: 0.6608\n",
            "Epoch [63/100], Batch [200/352], Loss: 0.5979\n",
            "Epoch [63/100], Batch [300/352], Loss: 0.8635\n",
            "Epoch 63/100 | Train Loss: 0.6924, Train Acc: 0.7559 | Val Loss: 0.7115, Val Acc: 0.7512\n",
            "Epoch [64/100], Batch [100/352], Loss: 0.5205\n",
            "Epoch [64/100], Batch [200/352], Loss: 0.7292\n",
            "Epoch [64/100], Batch [300/352], Loss: 0.7396\n",
            "Epoch 64/100 | Train Loss: 0.6950, Train Acc: 0.7557 | Val Loss: 0.6955, Val Acc: 0.7572\n",
            "Epoch [65/100], Batch [100/352], Loss: 0.6039\n",
            "Epoch [65/100], Batch [200/352], Loss: 0.7710\n",
            "Epoch [65/100], Batch [300/352], Loss: 0.5293\n",
            "Epoch 65/100 | Train Loss: 0.6900, Train Acc: 0.7586 | Val Loss: 0.6822, Val Acc: 0.7688\n",
            "Epoch [66/100], Batch [100/352], Loss: 0.7679\n",
            "Epoch [66/100], Batch [200/352], Loss: 0.9210\n",
            "Epoch [66/100], Batch [300/352], Loss: 0.5698\n",
            "Epoch 66/100 | Train Loss: 0.6910, Train Acc: 0.7577 | Val Loss: 0.6673, Val Acc: 0.7726\n",
            "Epoch [67/100], Batch [100/352], Loss: 0.8222\n",
            "Epoch [67/100], Batch [200/352], Loss: 0.4935\n",
            "Epoch [67/100], Batch [300/352], Loss: 0.6594\n",
            "Epoch 67/100 | Train Loss: 0.6934, Train Acc: 0.7565 | Val Loss: 0.7106, Val Acc: 0.7556\n",
            "Epoch [68/100], Batch [100/352], Loss: 0.8510\n",
            "Epoch [68/100], Batch [200/352], Loss: 0.5050\n",
            "Epoch [68/100], Batch [300/352], Loss: 0.6696\n",
            "Epoch 68/100 | Train Loss: 0.6907, Train Acc: 0.7580 | Val Loss: 0.6912, Val Acc: 0.7616\n",
            "Epoch [69/100], Batch [100/352], Loss: 0.5969\n",
            "Epoch [69/100], Batch [200/352], Loss: 0.6268\n",
            "Epoch [69/100], Batch [300/352], Loss: 0.7399\n",
            "Epoch 69/100 | Train Loss: 0.6905, Train Acc: 0.7575 | Val Loss: 0.6819, Val Acc: 0.7694\n",
            "Epoch [70/100], Batch [100/352], Loss: 0.6014\n",
            "Epoch [70/100], Batch [200/352], Loss: 0.6544\n",
            "Epoch [70/100], Batch [300/352], Loss: 0.7757\n",
            "Epoch 70/100 | Train Loss: 0.6854, Train Acc: 0.7597 | Val Loss: 0.6774, Val Acc: 0.7646\n",
            "Epoch [71/100], Batch [100/352], Loss: 0.7227\n",
            "Epoch [71/100], Batch [200/352], Loss: 1.0093\n",
            "Epoch [71/100], Batch [300/352], Loss: 0.8569\n",
            "Epoch 71/100 | Train Loss: 0.6847, Train Acc: 0.7582 | Val Loss: 0.6917, Val Acc: 0.7648\n",
            "Epoch [72/100], Batch [100/352], Loss: 0.6681\n",
            "Epoch [72/100], Batch [200/352], Loss: 0.6407\n",
            "Epoch [72/100], Batch [300/352], Loss: 0.5664\n",
            "Epoch 72/100 | Train Loss: 0.6818, Train Acc: 0.7592 | Val Loss: 0.6736, Val Acc: 0.7716\n",
            "Epoch [73/100], Batch [100/352], Loss: 0.6822\n",
            "Epoch [73/100], Batch [200/352], Loss: 0.8387\n",
            "Epoch [73/100], Batch [300/352], Loss: 0.6957\n",
            "Epoch 73/100 | Train Loss: 0.6794, Train Acc: 0.7597 | Val Loss: 0.6851, Val Acc: 0.7662\n",
            "Epoch [74/100], Batch [100/352], Loss: 0.6144\n",
            "Epoch [74/100], Batch [200/352], Loss: 0.6813\n",
            "Epoch [74/100], Batch [300/352], Loss: 0.7164\n",
            "Epoch 74/100 | Train Loss: 0.6836, Train Acc: 0.7596 | Val Loss: 0.6818, Val Acc: 0.7626\n",
            "Epoch [75/100], Batch [100/352], Loss: 0.6294\n",
            "Epoch [75/100], Batch [200/352], Loss: 0.6001\n",
            "Epoch [75/100], Batch [300/352], Loss: 0.8535\n",
            "Epoch 75/100 | Train Loss: 0.6774, Train Acc: 0.7646 | Val Loss: 0.6873, Val Acc: 0.7590\n",
            "Epoch [76/100], Batch [100/352], Loss: 0.6978\n",
            "Epoch [76/100], Batch [200/352], Loss: 0.6524\n",
            "Epoch [76/100], Batch [300/352], Loss: 0.7159\n",
            "Epoch 76/100 | Train Loss: 0.6809, Train Acc: 0.7611 | Val Loss: 0.6853, Val Acc: 0.7646\n",
            "Epoch [77/100], Batch [100/352], Loss: 0.7095\n",
            "Epoch [77/100], Batch [200/352], Loss: 0.8053\n",
            "Epoch [77/100], Batch [300/352], Loss: 0.6386\n",
            "Epoch 77/100 | Train Loss: 0.6733, Train Acc: 0.7647 | Val Loss: 0.6840, Val Acc: 0.7640\n",
            "Epoch [78/100], Batch [100/352], Loss: 0.6653\n",
            "Epoch [78/100], Batch [200/352], Loss: 0.7275\n",
            "Epoch [78/100], Batch [300/352], Loss: 0.6942\n",
            "Epoch 78/100 | Train Loss: 0.6780, Train Acc: 0.7624 | Val Loss: 0.6792, Val Acc: 0.7608\n",
            "Epoch [79/100], Batch [100/352], Loss: 0.6672\n",
            "Epoch [79/100], Batch [200/352], Loss: 0.7269\n",
            "Epoch [79/100], Batch [300/352], Loss: 0.6538\n",
            "Epoch 79/100 | Train Loss: 0.6786, Train Acc: 0.7599 | Val Loss: 0.6587, Val Acc: 0.7750\n",
            "Epoch [80/100], Batch [100/352], Loss: 0.7135\n",
            "Epoch [80/100], Batch [200/352], Loss: 0.5745\n",
            "Epoch [80/100], Batch [300/352], Loss: 0.5758\n",
            "Epoch 80/100 | Train Loss: 0.6676, Train Acc: 0.7675 | Val Loss: 0.6870, Val Acc: 0.7662\n",
            "Epoch [81/100], Batch [100/352], Loss: 0.6066\n",
            "Epoch [81/100], Batch [200/352], Loss: 0.6541\n",
            "Epoch [81/100], Batch [300/352], Loss: 0.7161\n",
            "Epoch 81/100 | Train Loss: 0.6685, Train Acc: 0.7661 | Val Loss: 0.6782, Val Acc: 0.7650\n",
            "Epoch [82/100], Batch [100/352], Loss: 0.7609\n",
            "Epoch [82/100], Batch [200/352], Loss: 0.7100\n",
            "Epoch [82/100], Batch [300/352], Loss: 0.6096\n",
            "Epoch 82/100 | Train Loss: 0.6726, Train Acc: 0.7631 | Val Loss: 0.6939, Val Acc: 0.7600\n",
            "Epoch [83/100], Batch [100/352], Loss: 0.6615\n",
            "Epoch [83/100], Batch [200/352], Loss: 0.6126\n",
            "Epoch [83/100], Batch [300/352], Loss: 0.6310\n",
            "Epoch 83/100 | Train Loss: 0.6796, Train Acc: 0.7610 | Val Loss: 0.6625, Val Acc: 0.7718\n",
            "Epoch [84/100], Batch [100/352], Loss: 0.5879\n",
            "Epoch [84/100], Batch [200/352], Loss: 0.6484\n",
            "Epoch [84/100], Batch [300/352], Loss: 0.5863\n",
            "Epoch 84/100 | Train Loss: 0.6750, Train Acc: 0.7634 | Val Loss: 0.6823, Val Acc: 0.7664\n",
            "Epoch [85/100], Batch [100/352], Loss: 0.6199\n",
            "Epoch [85/100], Batch [200/352], Loss: 0.5843\n",
            "Epoch [85/100], Batch [300/352], Loss: 0.6821\n",
            "Epoch 85/100 | Train Loss: 0.6686, Train Acc: 0.7657 | Val Loss: 0.6970, Val Acc: 0.7604\n",
            "Epoch [86/100], Batch [100/352], Loss: 0.6767\n",
            "Epoch [86/100], Batch [200/352], Loss: 0.6385\n",
            "Epoch [86/100], Batch [300/352], Loss: 0.6380\n",
            "Epoch 86/100 | Train Loss: 0.6733, Train Acc: 0.7636 | Val Loss: 0.6613, Val Acc: 0.7698\n",
            "Epoch [87/100], Batch [100/352], Loss: 0.6342\n",
            "Epoch [87/100], Batch [200/352], Loss: 0.6240\n",
            "Epoch [87/100], Batch [300/352], Loss: 0.6908\n",
            "Epoch 87/100 | Train Loss: 0.6696, Train Acc: 0.7651 | Val Loss: 0.6686, Val Acc: 0.7700\n",
            "Epoch [88/100], Batch [100/352], Loss: 0.8784\n",
            "Epoch [88/100], Batch [200/352], Loss: 0.7606\n",
            "Epoch [88/100], Batch [300/352], Loss: 0.5348\n",
            "Epoch 88/100 | Train Loss: 0.6644, Train Acc: 0.7657 | Val Loss: 0.6820, Val Acc: 0.7678\n",
            "Epoch [89/100], Batch [100/352], Loss: 0.5912\n",
            "Epoch [89/100], Batch [200/352], Loss: 0.6123\n",
            "Epoch [89/100], Batch [300/352], Loss: 0.6332\n",
            "Epoch 89/100 | Train Loss: 0.6682, Train Acc: 0.7669 | Val Loss: 0.6805, Val Acc: 0.7618\n",
            "Epoch [90/100], Batch [100/352], Loss: 0.8624\n",
            "Epoch [90/100], Batch [200/352], Loss: 0.6215\n",
            "Epoch [90/100], Batch [300/352], Loss: 0.7400\n",
            "Epoch 90/100 | Train Loss: 0.6673, Train Acc: 0.7679 | Val Loss: 0.6623, Val Acc: 0.7698\n",
            "Epoch [91/100], Batch [100/352], Loss: 0.6384\n",
            "Epoch [91/100], Batch [200/352], Loss: 0.7023\n",
            "Epoch [91/100], Batch [300/352], Loss: 0.6287\n",
            "Epoch 91/100 | Train Loss: 0.6650, Train Acc: 0.7671 | Val Loss: 0.6755, Val Acc: 0.7630\n",
            "Epoch [92/100], Batch [100/352], Loss: 0.7327\n",
            "Epoch [92/100], Batch [200/352], Loss: 0.6875\n",
            "Epoch [92/100], Batch [300/352], Loss: 0.5217\n",
            "Epoch 92/100 | Train Loss: 0.6582, Train Acc: 0.7669 | Val Loss: 0.6606, Val Acc: 0.7722\n",
            "Epoch [93/100], Batch [100/352], Loss: 0.6774\n",
            "Epoch [93/100], Batch [200/352], Loss: 0.6156\n",
            "Epoch [93/100], Batch [300/352], Loss: 0.5416\n",
            "Epoch 93/100 | Train Loss: 0.6622, Train Acc: 0.7666 | Val Loss: 0.6700, Val Acc: 0.7692\n",
            "Epoch [94/100], Batch [100/352], Loss: 0.7524\n",
            "Epoch [94/100], Batch [200/352], Loss: 0.6043\n",
            "Epoch [94/100], Batch [300/352], Loss: 0.5229\n",
            "Epoch 94/100 | Train Loss: 0.6615, Train Acc: 0.7660 | Val Loss: 0.6569, Val Acc: 0.7740\n",
            "Epoch [95/100], Batch [100/352], Loss: 0.6963\n",
            "Epoch [95/100], Batch [200/352], Loss: 0.7138\n",
            "Epoch [95/100], Batch [300/352], Loss: 0.5911\n",
            "Epoch 95/100 | Train Loss: 0.6586, Train Acc: 0.7675 | Val Loss: 0.6718, Val Acc: 0.7618\n",
            "Epoch [96/100], Batch [100/352], Loss: 0.6873\n",
            "Epoch [96/100], Batch [200/352], Loss: 0.5136\n",
            "Epoch [96/100], Batch [300/352], Loss: 0.7872\n",
            "Epoch 96/100 | Train Loss: 0.6605, Train Acc: 0.7683 | Val Loss: 0.6509, Val Acc: 0.7704\n",
            "Epoch [97/100], Batch [100/352], Loss: 0.7833\n",
            "Epoch [97/100], Batch [200/352], Loss: 0.6462\n",
            "Epoch [97/100], Batch [300/352], Loss: 0.6997\n",
            "Epoch 97/100 | Train Loss: 0.6634, Train Acc: 0.7679 | Val Loss: 0.6595, Val Acc: 0.7688\n",
            "Epoch [98/100], Batch [100/352], Loss: 0.6140\n",
            "Epoch [98/100], Batch [200/352], Loss: 0.6347\n",
            "Epoch [98/100], Batch [300/352], Loss: 0.6724\n",
            "Epoch 98/100 | Train Loss: 0.6586, Train Acc: 0.7681 | Val Loss: 0.6625, Val Acc: 0.7770\n",
            "Epoch [99/100], Batch [100/352], Loss: 0.6066\n",
            "Epoch [99/100], Batch [200/352], Loss: 0.6442\n",
            "Epoch [99/100], Batch [300/352], Loss: 0.7443\n",
            "Epoch 99/100 | Train Loss: 0.6564, Train Acc: 0.7701 | Val Loss: 0.6601, Val Acc: 0.7716\n",
            "Epoch [100/100], Batch [100/352], Loss: 0.8381\n",
            "Epoch [100/100], Batch [200/352], Loss: 0.6650\n",
            "Epoch [100/100], Batch [300/352], Loss: 0.7349\n",
            "Epoch 100/100 | Train Loss: 0.6576, Train Acc: 0.7701 | Val Loss: 0.6662, Val Acc: 0.7688\n",
            "--- Training Finished for Underfitting in 28.52 minutes ---\n",
            "\n",
            "=== Experiment 2: Baseline (Overfitting) ===\n",
            "\n",
            "--- Training Baseline ---\n",
            "Epoch [1/100], Batch [100/352], Loss: 1.7194\n",
            "Epoch [1/100], Batch [200/352], Loss: 1.7145\n",
            "Epoch [1/100], Batch [300/352], Loss: 1.4386\n",
            "Epoch 1/100 | Train Loss: 1.5326, Train Acc: 0.4252 | Val Loss: 1.2119, Val Acc: 0.5590\n",
            "Epoch [2/100], Batch [100/352], Loss: 1.1663\n",
            "Epoch [2/100], Batch [200/352], Loss: 1.0987\n",
            "Epoch [2/100], Batch [300/352], Loss: 1.0208\n",
            "Epoch 2/100 | Train Loss: 1.0790, Train Acc: 0.6149 | Val Loss: 1.0373, Val Acc: 0.6298\n",
            "Epoch [3/100], Batch [100/352], Loss: 0.9023\n",
            "Epoch [3/100], Batch [200/352], Loss: 0.9138\n",
            "Epoch [3/100], Batch [300/352], Loss: 0.8094\n",
            "Epoch 3/100 | Train Loss: 0.8714, Train Acc: 0.6910 | Val Loss: 0.7316, Val Acc: 0.7440\n",
            "Epoch [4/100], Batch [100/352], Loss: 0.7595\n",
            "Epoch [4/100], Batch [200/352], Loss: 0.9084\n",
            "Epoch [4/100], Batch [300/352], Loss: 0.8122\n",
            "Epoch 4/100 | Train Loss: 0.7561, Train Acc: 0.7340 | Val Loss: 0.7080, Val Acc: 0.7520\n",
            "Epoch [5/100], Batch [100/352], Loss: 0.5990\n",
            "Epoch [5/100], Batch [200/352], Loss: 0.5622\n",
            "Epoch [5/100], Batch [300/352], Loss: 0.8262\n",
            "Epoch 5/100 | Train Loss: 0.6693, Train Acc: 0.7645 | Val Loss: 0.6983, Val Acc: 0.7608\n",
            "Epoch [6/100], Batch [100/352], Loss: 0.5786\n",
            "Epoch [6/100], Batch [200/352], Loss: 0.6515\n",
            "Epoch [6/100], Batch [300/352], Loss: 0.5538\n",
            "Epoch 6/100 | Train Loss: 0.6053, Train Acc: 0.7898 | Val Loss: 0.6824, Val Acc: 0.7696\n",
            "Epoch [7/100], Batch [100/352], Loss: 0.5390\n",
            "Epoch [7/100], Batch [200/352], Loss: 0.5304\n",
            "Epoch [7/100], Batch [300/352], Loss: 0.5319\n",
            "Epoch 7/100 | Train Loss: 0.5660, Train Acc: 0.8031 | Val Loss: 0.5701, Val Acc: 0.8056\n",
            "Epoch [8/100], Batch [100/352], Loss: 0.7814\n",
            "Epoch [8/100], Batch [200/352], Loss: 0.6312\n",
            "Epoch [8/100], Batch [300/352], Loss: 0.5428\n",
            "Epoch 8/100 | Train Loss: 0.5212, Train Acc: 0.8197 | Val Loss: 0.5265, Val Acc: 0.8208\n",
            "Epoch [9/100], Batch [100/352], Loss: 0.4938\n",
            "Epoch [9/100], Batch [200/352], Loss: 0.4440\n",
            "Epoch [9/100], Batch [300/352], Loss: 0.5411\n",
            "Epoch 9/100 | Train Loss: 0.4861, Train Acc: 0.8316 | Val Loss: 0.5023, Val Acc: 0.8288\n",
            "Epoch [10/100], Batch [100/352], Loss: 0.4340\n",
            "Epoch [10/100], Batch [200/352], Loss: 0.5784\n",
            "Epoch [10/100], Batch [300/352], Loss: 0.4111\n",
            "Epoch 10/100 | Train Loss: 0.4544, Train Acc: 0.8418 | Val Loss: 0.5533, Val Acc: 0.8106\n",
            "Epoch [11/100], Batch [100/352], Loss: 0.5135\n",
            "Epoch [11/100], Batch [200/352], Loss: 0.4298\n",
            "Epoch [11/100], Batch [300/352], Loss: 0.3954\n",
            "Epoch 11/100 | Train Loss: 0.4285, Train Acc: 0.8530 | Val Loss: 0.4691, Val Acc: 0.8422\n",
            "Epoch [12/100], Batch [100/352], Loss: 0.2903\n",
            "Epoch [12/100], Batch [200/352], Loss: 0.3247\n",
            "Epoch [12/100], Batch [300/352], Loss: 0.3689\n",
            "Epoch 12/100 | Train Loss: 0.4061, Train Acc: 0.8616 | Val Loss: 0.4823, Val Acc: 0.8338\n",
            "Epoch [13/100], Batch [100/352], Loss: 0.3925\n",
            "Epoch [13/100], Batch [200/352], Loss: 0.3253\n",
            "Epoch [13/100], Batch [300/352], Loss: 0.3223\n",
            "Epoch 13/100 | Train Loss: 0.3912, Train Acc: 0.8653 | Val Loss: 0.4646, Val Acc: 0.8440\n",
            "Epoch [14/100], Batch [100/352], Loss: 0.2550\n",
            "Epoch [14/100], Batch [200/352], Loss: 0.4094\n",
            "Epoch [14/100], Batch [300/352], Loss: 0.3340\n",
            "Epoch 14/100 | Train Loss: 0.3668, Train Acc: 0.8728 | Val Loss: 0.4770, Val Acc: 0.8364\n",
            "Epoch [15/100], Batch [100/352], Loss: 0.2281\n",
            "Epoch [15/100], Batch [200/352], Loss: 0.2556\n",
            "Epoch [15/100], Batch [300/352], Loss: 0.3767\n",
            "Epoch 15/100 | Train Loss: 0.3545, Train Acc: 0.8778 | Val Loss: 0.4911, Val Acc: 0.8410\n",
            "Epoch [16/100], Batch [100/352], Loss: 0.3448\n",
            "Epoch [16/100], Batch [200/352], Loss: 0.4743\n",
            "Epoch [16/100], Batch [300/352], Loss: 0.3746\n",
            "Epoch 16/100 | Train Loss: 0.3372, Train Acc: 0.8841 | Val Loss: 0.4601, Val Acc: 0.8500\n",
            "Epoch [17/100], Batch [100/352], Loss: 0.1998\n",
            "Epoch [17/100], Batch [200/352], Loss: 0.4372\n",
            "Epoch [17/100], Batch [300/352], Loss: 0.4467\n",
            "Epoch 17/100 | Train Loss: 0.3173, Train Acc: 0.8887 | Val Loss: 0.4631, Val Acc: 0.8508\n",
            "Epoch [18/100], Batch [100/352], Loss: 0.2416\n",
            "Epoch [18/100], Batch [200/352], Loss: 0.3516\n",
            "Epoch [18/100], Batch [300/352], Loss: 0.2406\n",
            "Epoch 18/100 | Train Loss: 0.3066, Train Acc: 0.8935 | Val Loss: 0.4825, Val Acc: 0.8410\n",
            "Epoch [19/100], Batch [100/352], Loss: 0.2588\n",
            "Epoch [19/100], Batch [200/352], Loss: 0.3173\n",
            "Epoch [19/100], Batch [300/352], Loss: 0.2524\n",
            "Epoch 19/100 | Train Loss: 0.2982, Train Acc: 0.8955 | Val Loss: 0.4400, Val Acc: 0.8680\n",
            "Epoch [20/100], Batch [100/352], Loss: 0.3159\n",
            "Epoch [20/100], Batch [200/352], Loss: 0.2938\n",
            "Epoch [20/100], Batch [300/352], Loss: 0.2907\n",
            "Epoch 20/100 | Train Loss: 0.2879, Train Acc: 0.8994 | Val Loss: 0.4200, Val Acc: 0.8674\n",
            "Epoch [21/100], Batch [100/352], Loss: 0.3187\n",
            "Epoch [21/100], Batch [200/352], Loss: 0.3075\n",
            "Epoch [21/100], Batch [300/352], Loss: 0.2396\n",
            "Epoch 21/100 | Train Loss: 0.2791, Train Acc: 0.9037 | Val Loss: 0.3993, Val Acc: 0.8698\n",
            "Epoch [22/100], Batch [100/352], Loss: 0.2335\n",
            "Epoch [22/100], Batch [200/352], Loss: 0.3154\n",
            "Epoch [22/100], Batch [300/352], Loss: 0.2839\n",
            "Epoch 22/100 | Train Loss: 0.2678, Train Acc: 0.9050 | Val Loss: 0.3917, Val Acc: 0.8666\n",
            "Epoch [23/100], Batch [100/352], Loss: 0.2212\n",
            "Epoch [23/100], Batch [200/352], Loss: 0.2534\n",
            "Epoch [23/100], Batch [300/352], Loss: 0.2996\n",
            "Epoch 23/100 | Train Loss: 0.2599, Train Acc: 0.9094 | Val Loss: 0.4316, Val Acc: 0.8542\n",
            "Epoch [24/100], Batch [100/352], Loss: 0.2311\n",
            "Epoch [24/100], Batch [200/352], Loss: 0.2830\n",
            "Epoch [24/100], Batch [300/352], Loss: 0.2936\n",
            "Epoch 24/100 | Train Loss: 0.2456, Train Acc: 0.9152 | Val Loss: 0.4171, Val Acc: 0.8632\n",
            "Epoch [25/100], Batch [100/352], Loss: 0.3059\n",
            "Epoch [25/100], Batch [200/352], Loss: 0.1322\n",
            "Epoch [25/100], Batch [300/352], Loss: 0.2316\n",
            "Epoch 25/100 | Train Loss: 0.2430, Train Acc: 0.9138 | Val Loss: 0.4258, Val Acc: 0.8668\n",
            "Epoch [26/100], Batch [100/352], Loss: 0.3195\n",
            "Epoch [26/100], Batch [200/352], Loss: 0.2666\n",
            "Epoch [26/100], Batch [300/352], Loss: 0.2750\n",
            "Epoch 26/100 | Train Loss: 0.2309, Train Acc: 0.9199 | Val Loss: 0.4210, Val Acc: 0.8746\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "results = {} # To store history for plotting\n",
        "final_models = {} # To store trained models for final evaluation\n",
        "stopped_epochs = {} # To store where training stopped\n",
        "\n",
        "# --- Experiment 1: Underfitting ---\n",
        "print(\"\\n=== Experiment 1: Underfitting ===\")\n",
        "underfit_model = UnderfittingCNN(num_classes=10).to(DEVICE)\n",
        "optimizer_underfit = optim.Adam(underfit_model.parameters(), lr=LEARNING_RATE)\n",
        "final_models['Underfitting'], results['Underfitting'], stopped_epochs['Underfitting'] = train_model(\n",
        "    underfit_model, criterion, optimizer_underfit, train_loader, val_loader,\n",
        "    epochs=EPOCHS, device=DEVICE, model_name=\"Underfitting\"\n",
        ")\n",
        "\n",
        "# --- Experiment 2: Baseline (Potential Overfitting) ---\n",
        "print(\"\\n=== Experiment 2: Baseline (Overfitting) ===\")\n",
        "baseline_model = OverfittingCNN(num_classes=10).to(DEVICE)\n",
        "optimizer_baseline = optim.Adam(baseline_model.parameters(), lr=LEARNING_RATE, weight_decay=0) # No L2\n",
        "final_models['Baseline'], results['Baseline'], stopped_epochs['Baseline'] = train_model(\n",
        "    baseline_model, criterion, optimizer_baseline, train_loader, val_loader,\n",
        "    epochs=EPOCHS, device=DEVICE, l1_lambda=0.0, model_name=\"Baseline\" # No L1\n",
        ")\n",
        "\n",
        "# --- Experiment 3: L2 Regularization ---\n",
        "print(\"\\n=== Experiment 3: L2 Regularization ===\")\n",
        "l2_model = OverfittingCNN(num_classes=10).to(DEVICE)\n",
        "# AdamW applies weight decay correctly (decoupled weight decay)\n",
        "optimizer_l2 = optim.AdamW(l2_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY_L2)\n",
        "final_models['L2 Regularization'], results['L2 Regularization'], stopped_epochs['L2 Regularization'] = train_model(\n",
        "    l2_model, criterion, optimizer_l2, train_loader, val_loader,\n",
        "    epochs=EPOCHS, device=DEVICE, l1_lambda=0.0, model_name=\"L2 Regularization\" # No L1\n",
        ")\n",
        "\n",
        "# --- Experiment 4: L1 Regularization ---\n",
        "print(\"\\n=== Experiment 4: L1 Regularization ===\")\n",
        "l1_model = OverfittingCNN(num_classes=10).to(DEVICE)\n",
        "# Use standard Adam, L1 is added manually in the training loop\n",
        "optimizer_l1 = optim.Adam(l1_model.parameters(), lr=LEARNING_RATE, weight_decay=0) # No L2 here\n",
        "final_models['L1 Regularization'], results['L1 Regularization'], stopped_epochs['L1 Regularization'] = train_model(\n",
        "    l1_model, criterion, optimizer_l1, train_loader, val_loader,\n",
        "    epochs=EPOCHS, device=DEVICE, l1_lambda=L1_LAMBDA, model_name=\"L1 Regularization\"\n",
        ")\n",
        "\n",
        "# --- Experiment 5: Elastic Net Regularization ---\n",
        "print(\"\\n=== Experiment 5: Elastic Net Regularization ===\")\n",
        "elastic_model = OverfittingCNN(num_classes=10).to(DEVICE)\n",
        "# Use AdamW for L2 part, L1 is added manually\n",
        "optimizer_elastic = optim.AdamW(elastic_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY_L2)\n",
        "final_models['Elastic Net'], results['Elastic Net'], stopped_epochs['Elastic Net'] = train_model(\n",
        "    elastic_model, criterion, optimizer_elastic, train_loader, val_loader,\n",
        "    epochs=EPOCHS, device=DEVICE, l1_lambda=L1_LAMBDA, model_name=\"Elastic Net\"\n",
        ")\n",
        "\n",
        "# --- Experiment 6: Early Stopping ---\n",
        "print(\"\\n=== Experiment 6: Early Stopping ===\")\n",
        "early_stop_model = OverfittingCNN(num_classes=10).to(DEVICE)\n",
        "optimizer_early_stop = optim.Adam(early_stop_model.parameters(), lr=LEARNING_RATE, weight_decay=0) # No L2\n",
        "final_models['Early Stopping'], results['Early Stopping'], stopped_epochs['Early Stopping'] = train_model(\n",
        "    early_stop_model, criterion, optimizer_early_stop, train_loader, val_loader,\n",
        "    epochs=EPOCHS, device=DEVICE, l1_lambda=0.0, # No L1\n",
        "    early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
        "    early_stopping_min_delta=EARLY_STOPPING_MIN_DELTA,\n",
        "    model_name=\"Early Stopping\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1gvXlspPeJw"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Plotting Results ---\")\n",
        "\n",
        "def plot_history(results, stopped_epochs, title_prefix=\"\"):\n",
        "    \"\"\"Plots training and validation loss and accuracy.\"\"\"\n",
        "    plt.style.use('seaborn-v0_8-darkgrid') # Pretty plots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
        "    fig.suptitle(f'{title_prefix} Training History Comparison', fontsize=16)\n",
        "\n",
        "    # Plot Loss\n",
        "    for name, history in results.items():\n",
        "        epochs_ran = stopped_epochs[name]\n",
        "        epochs_axis = range(1, epochs_ran + 1)\n",
        "        axs[0].plot(epochs_axis, history['train_loss'][:epochs_ran], label=f'{name} Train Loss')\n",
        "        axs[0].plot(epochs_axis, history['val_loss'][:epochs_ran], label=f'{name} Val Loss', linestyle='--')\n",
        "    axs[0].set_title('Loss per Epoch')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].legend(loc='best')\n",
        "    axs[0].grid(True)\n",
        "    axs[0].set_ylim(bottom=0) # Loss cannot be negative\n",
        "\n",
        "    # Plot Accuracy\n",
        "    for name, history in results.items():\n",
        "        epochs_ran = stopped_epochs[name]\n",
        "        epochs_axis = range(1, epochs_ran + 1)\n",
        "        axs[1].plot(epochs_axis, history['train_acc'][:epochs_ran], label=f'{name} Train Acc')\n",
        "        axs[1].plot(epochs_axis, history['val_acc'][:epochs_ran], label=f'{name} Val Acc', linestyle='--')\n",
        "    axs[1].set_title('Accuracy per Epoch')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].legend(loc='best')\n",
        "    axs[1].grid(True)\n",
        "    axs[1].set_ylim(0, 1.05) # Accuracy between 0 and 1\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "    plt.show()\n",
        "\n",
        "# Plot all results together\n",
        "plot_history(results, stopped_epochs)\n",
        "\n",
        "# Optionally, plot specific comparisons if the main plot is too crowded\n",
        "# Example: Baseline vs L2 vs Early Stopping\n",
        "plot_history({k: results[k] for k in ['Baseline', 'L2 Regularization', 'Early Stopping']},\n",
        "              {k: stopped_epochs[k] for k in ['Baseline', 'L2 Regularization', 'Early Stopping']},\n",
        "              title_prefix=\"Baseline vs L2 vs Early Stopping\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuljyuwfPhdi"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Final Evaluation on Test Set ---\")\n",
        "\n",
        "evaluation_summary = []\n",
        "\n",
        "for name, model in final_models.items():\n",
        "    print(f\"Evaluating {name}...\")\n",
        "    test_loss, test_acc = evaluate_model(model, test_loader, criterion, DEVICE)\n",
        "    # Get more detailed report (optional, can be slow if test set is huge)\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "    report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True)\n",
        "    test_acc_from_report = report['accuracy'] # Should match test_acc\n",
        "\n",
        "    evaluation_summary.append({\n",
        "        \"Model\": name,\n",
        "        \"Test Loss\": test_loss,\n",
        "        \"Test Accuracy\": test_acc,\n",
        "        \"Stopped Epoch\": stopped_epochs[name]\n",
        "    })\n",
        "    print(f\"{name} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Stopped Epoch: {stopped_epochs[name]}\")\n",
        "\n",
        "\n",
        "# Create and print a Pandas DataFrame for a nice table\n",
        "evaluation_df = pd.DataFrame(evaluation_summary)\n",
        "print(\"\\n--- Evaluation Summary Table ---\")\n",
        "print(evaluation_df.to_string(index=False))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN3H3LFz1PfsVeNy9Hw+aw5",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
